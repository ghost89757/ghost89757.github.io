<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>loudmute - 一直游到，海水变蓝</title><meta name="author" content="loudmute,2779477538@qq.com"><meta name="copyright" content="loudmute"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一直游到，海水变蓝">
<meta property="og:type" content="website">
<meta property="og:title" content="loudmute">
<meta property="og:url" content="https://ghost89757.github.io/page/3/index.html">
<meta property="og:site_name" content="loudmute">
<meta property="og:description" content="一直游到，海水变蓝">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ghost89757.github.io/img/head-1.jpg">
<meta property="article:author" content="loudmute">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ghost89757.github.io/img/head-1.jpg"><link rel="shortcut icon" href="/img/cat-11.png"><link rel="canonical" href="https://ghost89757.github.io/page/3/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'loudmute',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2025-02-20 17:28:23'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/transpancy.css"><link rel="stylesheet" href="/css/modify.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/head-1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background: transparent"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">loudmute</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">loudmute</h1><div id="site-subtitle"><span id="subtitle"></span></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/post/1ded95c1.html" title="王爽《汇编语言》总结"><img class="post_bg" src="/img/7.jpg" onerror="this.onerror=null;this.src='/img/p6.png'" alt="王爽《汇编语言》总结"></a></div><div class="recent-post-info"><a class="article-title" href="/post/1ded95c1.html" title="王爽《汇编语言》总结">王爽《汇编语言》总结</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-10-01T00:47:48.000Z" title="发表于 2023-10-01 08:47:48">2023-10-01</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E7%BC%96%E7%A8%8B/">编程</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E7%BC%96%E7%A8%8B/%E6%B1%87%E7%BC%96/">汇编</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E7%BC%96%E7%A8%8B/">编程</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E6%B1%87%E7%BC%96/">汇编</a></span></div><div class="content">前言起因是因为操作系统，本来是打算跟南大JYY的操作系统课，上了两节，md听不懂，只能感到自己像个弱智。于是又去跟哈工大LZZ的课，还是听不懂，细究起来主要是他们讲汇编的时候，我没学过，一脸懵逼。网上有帖子说，学之前可以学一下王爽的《汇编语言》，于是搜刮了一些资料。一开始，我是抱着速通去的，花了三四天看完了这本书，然后接着听课，结果是囫囵吞枣、一事无成。痛定思痛，我又花了一周半的时间，好好学了这本《汇编语言》。不得不说，这本书写的太好了，完全就是让你可以自己从头看到尾，当然，最重要的是书里的代码，实验的代码，都写了一遍。你可能不信，我甚至在写汇编指令的时候感到了前所未有的快感，既然好好学了，就写个笔记吧，于是就有了这篇总结。
这篇总结淡化了大部分的硬件知识，以及大部分的文字，重点突出指令和我认为有意思的代码，又由于懒得重新构思大纲，就按着作者的原书架构来总结，原书一共17章，由于我希望这篇总结像指令大全一样可以速查，所以我只总结了重要的指令部分以及相关代码，原书的1、16、17章并没有介绍重要的指令，所以就省略了。
一-基础知识略…
二-寄存器（指令访问）1 几条汇编指令12345mo ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/post/63fbc417.html" title="无监督自适应（UDA）：让网络模型学会举一反三"><img class="post_bg" src="/img/6.jpg" onerror="this.onerror=null;this.src='/img/p6.png'" alt="无监督自适应（UDA）：让网络模型学会举一反三"></a></div><div class="recent-post-info"><a class="article-title" href="/post/63fbc417.html" title="无监督自适应（UDA）：让网络模型学会举一反三">无监督自适应（UDA）：让网络模型学会举一反三</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-07-14T08:07:47.000Z" title="发表于 2023-07-14 16:07:47">2023-07-14</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E8%87%AA%E9%80%82%E5%BA%94/">无监督自适应</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="content">图床：https://imgur.com/a/qRGFWjE
（看不到图片，可能是网络原因）
一  简述目前的深度学习真的会学习吗？
且不说网络模型，我们先思考一下我们自己到底会不会学习。请想想一下考试周的你，喝着热带风味的冰红茶，被焦躁的备考氛围所笼罩，看着老师强调的重点知识，刷着PPT或往年的试题，在刷了很多题后，你终于上考场了，拿到试卷，你不仅嘴角上扬，心想：“这题我都做过”，三十分钟，你便解决了战斗；你以为这就是学习的最好方法，接着不停的刷题，备考下场考试，你终于又上考场了，信心十足，因为你偷偷地背着室友把历年的试题全刷完了，然而拿到试卷，你慌了。这次的考试，并没有以前刷的那些原题，知识还是那个知识，题型却变了，走出考场，你怆然泪下，你付出了大量的时间和精力，把往年题都刷完了，你确实很努力，但好像知识在单纯的“背题”，并没有学习题背后的知识。
假如把人比作一个个网络模型，那些在考试周“背题”的人大概就是一个个深度学习网络模型，根据其花费时间精力、刷题数目可以分为不同的复杂网络模型，但无外乎输入题目，对比标签，优化网络，预测考试题的答案，一旦题型变化，狼狈不堪；而有的学神，刷题量 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/post/5b2f0628.html" title="MemoryAdaptNet：基于不变域级原型记忆的无监督领域自适应遥感语义分割"><img class="post_bg" src="/img/5.jpg" onerror="this.onerror=null;this.src='/img/p6.png'" alt="MemoryAdaptNet：基于不变域级原型记忆的无监督领域自适应遥感语义分割"></a></div><div class="recent-post-info"><a class="article-title" href="/post/5b2f0628.html" title="MemoryAdaptNet：基于不变域级原型记忆的无监督领域自适应遥感语义分割">MemoryAdaptNet：基于不变域级原型记忆的无监督领域自适应遥感语义分割</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-07-12T03:18:15.000Z" title="发表于 2023-07-12 11:18:15">2023-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/">语义分割</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E9%81%A5%E6%84%9F%E5%9B%BE%E5%83%8F/">遥感图像</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E8%87%AA%E9%80%82%E5%BA%94/">无监督自适应</a></span></div><div class="content">图床：https://imgur.com/a/Ocb88Lz
（看不到图片，可能是网络原因）



题目：《Unsupervised Domain Adaptation Semantic Segmentation of High-Resolution Remote Sensing Imagery With Invariant Domain-Level Prototype Memory》

github：https://github.com/RS-CSU/MemoryAdaptNet-master


引言语义分割是高分辨率遥感图像自编码的关键技术，并且在遥感领域备受关注。深度卷积网络由于其分层次的特征表达能力，已经成功应用于遥感分割任务。
然而，深度卷积网络的局限性：1.对大量标签标注数据的依赖；2.对数据分布差异的敏感，限制了卷积神经网络在遥感分割领域的应用。
本文提出了一种用于遥感语义分割的无监督自适应语义分割网络：MemoryAdaptNet。
I. INTRODUCTION随着地球观测技术的快速发展，高分辨率遥感图像越来越多，这位地球观测提供了可能。高分辨率遥感图像的自译码在遥 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/post/e0d2147a.html" title="Transformer"><img class="post_bg" src="/img/4.jpg" onerror="this.onerror=null;this.src='/img/p6.png'" alt="Transformer"></a></div><div class="recent-post-info"><a class="article-title" href="/post/e0d2147a.html" title="Transformer">Transformer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-07-12T01:05:43.000Z" title="发表于 2023-07-12 09:05:43">2023-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">注意力机制</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a></span></div><div class="content">图床：https://imgur.com/a/Ae6T9zo
（看不到图片，可能是网络原因）
一 前言RNN在机器翻译时，用于其顺序结构的影响，无法有效地联系前后文，并快速准确地进行翻译；由此，Attention机制被提出，有效地联系到了上下文，考虑到了全局地信息。
既然如此，何不舍弃RNN结构，用Attenion机制构造新的模型。于是，随着Google一篇名为《Attention is All you need》的论文，Transfoemer架构横空出世。
首先，我们给出Transformer结构的原图：


乍一看，甚是懵逼，所以，我们进行一个剖析。


起初，Tansformer就像一个黑盒，输入中文”人工智能”输出英文”Artificial Intelligence”。


细看，Tansformer由两个部分组成，即编码器和解码器。


实验中，编码器部分有6个编码器，解码器部分有6个解码器。


我们看每一个编码器，它由Multi-Head Attention和前馈网络两部分组成。
再看每一个编码器，它由Multi-Head Attention和Encoder-Decode ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/post/89b2e373.html" title="Attention：注意力机制"><img class="post_bg" src="/img/3.jpg" onerror="this.onerror=null;this.src='/img/p6.png'" alt="Attention：注意力机制"></a></div><div class="recent-post-info"><a class="article-title" href="/post/89b2e373.html" title="Attention：注意力机制">Attention：注意力机制</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-07-12T00:47:35.000Z" title="发表于 2023-07-12 08:47:35">2023-07-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">注意力机制</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Attention/">Attention</a></span></div><div class="content">图床：https://imgur.com/a/ztP7vUV
（看不到图片，可能是网络原因）
一 前言本篇的Attention剑指Transformer。
二 Encoder-Decoder1.Encoder—Decoder简介
Encoder-Decoder就是编码-解码框架
大部分的Attention机制模型都是依附于Encoder-Decoder框架进行实现
在NLP中Encoder-Decoder框架主要是被用来处理序列-序列问题
基于Encoder-Decoder框架，大多用seq2seq模型和Transformer模型实现

2.Encoder-Decoder结构原理


Encoder：是编码器，对于输入的序列&lt;x1,x2,x3…xn&gt;进行编码，使其转化为一个语义编码C，这个C中就储存了序列&lt;x1,x2,x3…xn&gt;的信息。

Encoder如何编码：

我们知道，Encoder要处理的数据是序列数据，而处理序列数据有很好效果的是循环神经网络（RNN/LSTM/GRU），所以Encoder可以采用循环神经网络的模型来进行编码。
 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/post/1ab40edc.html" title="3D Conv:立体卷积"><img class="post_bg" src="/img/2.jpg" onerror="this.onerror=null;this.src='/img/p6.png'" alt="3D Conv:立体卷积"></a></div><div class="recent-post-info"><a class="article-title" href="/post/1ab40edc.html" title="3D Conv:立体卷积">3D Conv:立体卷积</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-07-11T14:01:28.000Z" title="发表于 2023-07-11 22:01:28">2023-07-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">卷积神经网络</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">卷积神经网络</a></span></div><div class="content">图床：https://imgur.com/a/1G4v05r
（看不到图片，可能是网络原因）
一 从 2D卷积到3D卷积1.1 2D单通道卷积

如图，这是一个2D单通道卷积，我们重点看卷积的维度。
这里的输入图像有两个维度：高度h，宽度w；卷积核大小也有两个维度：高度h，宽度w；最终输出图像是二维的，也有两个维度：高度h，宽度w。
1.2 2D多通道卷积

如图，这是一个2D多通道卷积。输入图像有三个维度：高度h，宽度w，通道数c；卷积核也有三个维度：高度k，宽度k，通道数c；最终输出图像是二维的，有两个维度：高度h，宽度w。
这里我们可以把输入图像看作一个高度为h，宽为w，长为c的立方体，卷积核看作一个高度为k，宽度为k，长度为c的立方体。卷积核立方体再输入图像的立方体上面做二维的滑动运动，输出的是一个二维的图像。
运算过程可以参考下图，


1.3 3D单通道卷积

无论是视频还是3D图像，实际上都是对二维图像的堆叠，来达到现实中三维的效果。
假设我们这里输入的是一个视频，那么有层数为红、绿、蓝三层可以理解为我们的视频图像是RGB图像，而每个颜色层有也有三个图片可以理解为是三个R ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/post/e1675ab9.html" title="RNN:循环神经网络"><img class="post_bg" src="/img/1.jpg" onerror="this.onerror=null;this.src='/img/p6.png'" alt="RNN:循环神经网络"></a></div><div class="recent-post-info"><a class="article-title" href="/post/e1675ab9.html" title="RNN:循环神经网络">RNN:循环神经网络</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-07-11T13:53:21.000Z" title="发表于 2023-07-11 21:53:21">2023-07-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">循环神经网络</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">循环神经网络</a></span></div><div class="content">图床：https://imgur.com/a/SbrTFhC
（看不到图片，可能是网络原因）
一 什么是RNN
循环神经网络（Rerrent Neural Network）是神经网络中的一种，常用于NLP领域的问题。
特点：对具有序列特性的数据很有效，能够挖掘数据中的时序信息以及语义信息。
什么是序列特性：符合时间顺序、逻辑顺序或者其他顺序就叫序列特性

二 为什么提出RNN全连接网络无法结合上下文去训练模型。
三 循环神经网络的结构及原理

1.抛开W上图乍一看较为懵逼，所以先从认识的网络来看，抛开W不看，我们发现这就是一个全连接网络。
这样看：

X：输入层，就是一个向量，也就是某个字或词的特征向量
U：U就是输入层到输出层的参数矩阵
S：隐藏层
V：隐藏层到输出层的参数矩阵
O：输出层的向量

2.W是啥？能动左边的图，我们再看右边的W到底是啥。
我们把上图展开：


欧克，按时间线展开后，我们看了又是一脸懵逼，别急，听我详解。

我们把w的箭头看作是时间的流向，设当前时刻为t，那么上一时刻就是t - 1，下一时刻就是t + 1，所以，上图展开后，以W为时间流向，我们分别得到了从 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/post/154c7ef1.html" title="3D UX-Net：用于医学图像分割的大核体卷积现代化分层Transformer"><img class="post_bg" src="/img/10.jpg" onerror="this.onerror=null;this.src='/img/p6.png'" alt="3D UX-Net：用于医学图像分割的大核体卷积现代化分层Transformer"></a></div><div class="recent-post-info"><a class="article-title" href="/post/154c7ef1.html" title="3D UX-Net：用于医学图像分割的大核体卷积现代化分层Transformer">3D UX-Net：用于医学图像分割的大核体卷积现代化分层Transformer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-07-08T02:39:36.000Z" title="发表于 2023-07-08 10:39:36">2023-07-08</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Transformer/">Transformer</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></span></div><div class="content">图床：https://imgur.com/a/en5Mtdj
（看不到图片，可能是你的网络没用魔法）


标题：《3D UX-Net: a Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation》
Github：https://link.zhihu.com/?target=https%3A//github.com/MASILab/3DUX-Net
一 引言
大多数医学图像如MRI和CT都属于volumetric data（体积数据），为了更加充分利用体素信息，近几年提出了不少3D卷积模型，也取得一些进展。

近几年，随着Transformer的横空出世，其惊人的效果不仅让其在NLP领域火爆，Vision Transformer的提出也让Transformer在图像领域大展拳脚，Convnet在图像领域的地位受到挑战。于是有人开始反击，想证明Convnet可以做出Transformer的效果，于是ConvNext对标SwinTransformer ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/post/4269cbfe.html" title="Numpy总结-6.伪随机函数的生成"><img class="post_bg" src="/img/9.jpg" onerror="this.onerror=null;this.src='/img/p6.png'" alt="Numpy总结-6.伪随机函数的生成"></a></div><div class="recent-post-info"><a class="article-title" href="/post/4269cbfe.html" title="Numpy总结-6.伪随机函数的生成">Numpy总结-6.伪随机函数的生成</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-07-07T09:56:30.000Z" title="发表于 2023-07-07 17:56:30">2023-07-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E7%BC%96%E7%A8%8B/">编程</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E7%BC%96%E7%A8%8B/python/">python</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E7%BC%96%E7%A8%8B/python/Numpy/">Numpy</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E7%BC%96%E7%A8%8B/">编程</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/python/">python</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Numpy/">Numpy</a></span></div><div class="content">6. 伪随机函数的生成主要使用np.random模块
numpy.random中的常见函数（不全）：

seed 像随机数生成器传递随机状态种子
shuffle 随机排列一个序列
rand 从均匀分布中抽取样本
randint 由低到高，随机抽取整数
randn 从均值为0， 方差为1的正态分布中抽取样本
binomial 从二项分布中抽取样本
normal 从正态（高斯）分布中抽取样本

1import numpy as np


12samples = np.random.normal(size = (4, 4))samples




array([[ 0.10346038, -0.23277967,  0.23074895, -0.51645475],
       [ 0.88840995,  0.27574673, -1.0508378 ,  1.26257317],
       [-1.78183817,  0.18499594, -0.22393833,  1.36331696],
       [-0.08946206,  0.88673875, -1.272820 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/post/c2047baa.html" title="Numpy总结-5.线性代数"><img class="post_bg" src="/img/8.jpg" onerror="this.onerror=null;this.src='/img/p6.png'" alt="Numpy总结-5.线性代数"></a></div><div class="recent-post-info"><a class="article-title" href="/post/c2047baa.html" title="Numpy总结-5.线性代数">Numpy总结-5.线性代数</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-07-07T09:55:49.000Z" title="发表于 2023-07-07 17:55:49">2023-07-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E7%BC%96%E7%A8%8B/">编程</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E7%BC%96%E7%A8%8B/python/">python</a><i class="fas fa-angle-right article-meta-link"></i><a class="article-meta__categories" href="/categories/%E7%BC%96%E7%A8%8B/python/Numpy/">Numpy</a></span><span class="article-meta tags"><span class="article-meta-separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/%E7%BC%96%E7%A8%8B/">编程</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/python/">python</a><span class="article-meta-link">•</span><a class="article-meta__tags" href="/tags/Numpy/">Numpy</a></span></div><div class="content">5. 线性代数矩阵的逐元素乘积：*
矩阵的点乘：x.dot(y) np.dot(x, y) x @ y
numpy.linalg拥有一个矩阵分解的标准函数集，以及其他常用函数常用的numpy.linalg函数(不全)：

diag 将一个矩阵的对角元素作为一维数组返回，或将一个一维数组转换成方阵，并且非对角巷上位0
dot 矩阵的点乘
trace 计算矩阵对角元素和
det 计算矩阵的行列式
eig 计算方阵的特征值和特征向量
inv 计算方阵的逆矩阵
solve 求解方程Ax = b，其中A为方阵

1import numpy as np


123x = np.arange(1, 7).reshape(2, 3)y = np.array([[6, 23], [-1, 7], [8, 9]])x, y




(array([[1, 2, 3],
        [4, 5, 6]]),
 array([[ 6, 23],
        [-1,  7],
        [ 8,  9]]))

1x.dot(y), np.dot(x, y), x @ y




(a ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/2/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/#content-inner">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/page/4/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/head-1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">loudmute</div><div class="author-info__description">一直游到，海水变蓝</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">37</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ghost89757"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/e422cb2d.html" title="UnityNote5-UI系统">UnityNote5-UI系统</a><time datetime="2025-02-20T02:50:00.000Z" title="发表于 2025-02-20 10:50:00">2025-02-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/c5634d79.html" title="UnityNote4-数学基础">UnityNote4-数学基础</a><time datetime="2025-02-11T13:52:45.000Z" title="发表于 2025-02-11 21:52:45">2025-02-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/77bff30b.html" title="UnityNote3-物理系统">UnityNote3-物理系统</a><time datetime="2025-02-08T10:08:29.000Z" title="发表于 2025-02-08 18:08:29">2025-02-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/27a89f23.html" title="UnityNote2-基本概念和脚本编程">UnityNote2-基本概念和脚本编程</a><time datetime="2025-02-05T13:45:47.000Z" title="发表于 2025-02-05 21:45:47">2025-02-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/9fcb3586.html" title="UnityNote1-初识Unity">UnityNote1-初识Unity</a><time datetime="2025-01-17T08:38:05.000Z" title="发表于 2025-01-17 16:38:05">2025-01-17</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            <a class="card-more-btn" href="/categories/" title="查看更多">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"><span class="card-category-list-name">人工智能</span><span class="card-category-list-count">9</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="card-category-list-name">机器学习</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="card-category-list-name">深度学习</span><span class="card-category-list-count">5</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="card-category-list-name">卷积神经网络</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="card-category-list-name">循环神经网络</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"><span class="card-category-list-name">注意力机制</span><span class="card-category-list-count">2</span></a></li></ul></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="card-category-list-name">论文笔记</span><span class="card-category-list-count">3</span></a></li></ul></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"><span class="card-category-list-name">操作系统</span><span class="card-category-list-count">12</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/Attention/" style="font-size: 1.1em; color: #999">Attention</a> <a href="/tags/C/" style="font-size: 1.21em; color: #999ea4">C#</a> <a href="/tags/Numpy/" style="font-size: 1.33em; color: #99a2af">Numpy</a> <a href="/tags/Transformer/" style="font-size: 1.16em; color: #999b9e">Transformer</a> <a href="/tags/Unity/" style="font-size: 1.21em; color: #999ea4">Unity</a> <a href="/tags/Unity-%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91-C/" style="font-size: 1.16em; color: #999b9e">Unity - 游戏开发 - C#</a> <a href="/tags/python/" style="font-size: 1.33em; color: #99a2af">python</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 1.44em; color: #99a7ba">人工智能</a> <a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 1.1em; color: #999">卷积神经网络</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/" style="font-size: 1.16em; color: #999b9e">图像分割</a> <a href="/tags/%E5%AE%9E%E9%AA%8C/" style="font-size: 1.27em; color: #99a0a9">实验</a> <a href="/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 1.1em; color: #999">循环神经网络</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 1.5em; color: #99a9bf">操作系统</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E8%87%AA%E9%80%82%E5%BA%94/" style="font-size: 1.16em; color: #999b9e">无监督自适应</a> <a href="/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94/" style="font-size: 1.1em; color: #999">无监督领域自适应</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 1.1em; color: #999">机器学习</a> <a href="/tags/%E6%B1%87%E7%BC%96/" style="font-size: 1.1em; color: #999">汇编</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.33em; color: #99a2af">深度学习</a> <a href="/tags/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/" style="font-size: 1.21em; color: #999ea4">游戏开发</a> <a href="/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 1.39em; color: #99a4b4">笔记</a> <a href="/tags/%E7%BC%96%E7%A8%8B/" style="font-size: 1.39em; color: #99a4b4">编程</a> <a href="/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/" style="font-size: 1.16em; color: #999b9e">语义分割</a> <a href="/tags/%E9%81%A5%E6%84%9F%E5%9B%BE%E5%83%8F/" style="font-size: 1.1em; color: #999">遥感图像</a> <a href="/tags/%E9%9A%8F%E4%BE%BF%E5%86%99%E5%86%99/" style="font-size: 1.21em; color: #999ea4">随便写写</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/02/"><span class="card-archive-list-date">二月 2025</span><span class="card-archive-list-count">4</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/01/"><span class="card-archive-list-date">一月 2025</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">十二月 2024</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/11/"><span class="card-archive-list-date">十一月 2024</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">十一月 2023</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">十月 2023</span><span class="card-archive-list-count">12</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/07/"><span class="card-archive-list-date">七月 2023</span><span class="card-archive-list-count">16</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">37</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">93.8k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastpushdate="2025-02-20T09:28:22.837Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">©2022 - 2025 By loudmute</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>function subtitleType () {
  if (true) { 
    window.typed = new Typed("#subtitle", {
      strings: ["一直游到","海水变蓝"],
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50
    })
  } else {
    document.getElementById("subtitle").innerHTML = '一直游到'
  }
}

if (true) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.jsdelivr.net/npm/typed.js/lib/typed.min.js').then(subtitleType)
  }
} else {
  subtitleType()
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":120,"height":240,"hOffset":0,"vOffset":-45},"mobile":{"show":true},"react":{"opacity":1},"log":false});</script></body></html>